{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK 1:  CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook create cluster of MIGROS buildings accoring to their energy consumption patterns using ML unsupervised clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"ticks\", font_scale=1.2, context=\"talk\")\n",
    "sns.set_style(\"white\", {'axes.grid' : False})\n",
    "plt.rcParams['figure.figsize'] = (18, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migros_FULL_data = pd.read_csv(\"data/191126_E3M_LESO_GMOSRawValuesWithRanking.csv\",delimiter=\",\",header=0,encoding=\"ISO-8859-1\")\n",
    "\n",
    "# === Creating electricity time series =========\n",
    "# Select only the electricity demand (Stromverbrauch Gesamt = Total electricity consumption)\n",
    "# and add the time stamps columns at the beginning\n",
    "lol = migros_FULL_data.loc[0,:]\n",
    "good_columns = ['DPName'] + list(lol[lol==\"Stromverbrauch Gesamt\"].index)\n",
    "migros_FULL_data_elec = migros_FULL_data[good_columns]\n",
    "\n",
    "# fill the Nans in a forward way\n",
    "#migros_FULL_data_elec = migros_FULL_data_elec.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Station_locations_MIGROS = np.array(migros_FULL_data_elec.iloc[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# separate the series only and set the time stamps as the index of the series\n",
    "migros_FRAME = migros_FULL_data_elec.iloc[3:,:]\n",
    "migros_FRAME.index = pd.DatetimeIndex(migros_FRAME['DPName'])\n",
    "\n",
    "# to keep track of the location names, add them to the frame for now\n",
    "locations_frame = pd.DataFrame(Station_locations_MIGROS).T\n",
    "locations_frame.columns = migros_FRAME.columns\n",
    "migros_FRAME2 = pd.concat([locations_frame,migros_FRAME])\n",
    "\n",
    "# drop the old time stamps column and other random columns with names instead of values....\n",
    "migros_FRAME2 = migros_FRAME2.drop('DPName', 1)\n",
    "migros_FRAME2 = migros_FRAME2.drop('LESO_WTWU_EV_ELE', 1)\n",
    "migros_FRAME2 = migros_FRAME2.drop('WTOW_EV_ELE', 1)\n",
    "migros_FRAME2 = migros_FRAME2.drop('LESO_APPE_EV_ELE', 1)\n",
    "migros_FRAME2 = migros_FRAME2.drop('LESO_CHFT_EV_ELE', 1)\n",
    "migros_FRAME2 = migros_FRAME2.drop('LESO_ILAN_EV_ELE', 1)\n",
    "\n",
    "# now extract the location names after droping weird columns\n",
    "Station_locations_MIGROS2 = list(migros_FRAME2.iloc[0])\n",
    "\n",
    "# now take back the location names from the frame itself, and rename the original name!\n",
    "migros_FRAME = migros_FRAME2.drop([0])\n",
    "\n",
    "# put back in datetime\n",
    "migros_FRAME.index = pd.DatetimeIndex(migros_FRAME.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migros_FRAME.columns = Station_locations_MIGROS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migros_FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"OBI\", \"MFIT\",\"MFit\",\"Fitnesspark\", \"NEU\",\"Neu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Station_locations_MIGROS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now transforming strings into numbers. \n",
    "## A GOOD OLD CLASSIC, with the nice to_numeric function, that we APPLY to all columns with the equally nice apply function.\n",
    "\n",
    "migros_FRAME[migros_FRAME.columns] = migros_FRAME[migros_FRAME.columns].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "migros_FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migros_FRAME.to_csv(\"Migros_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fourier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregating them to a daily mean hourly\n",
    "migros_FRAME_daily = migros_FRAME.resample(\"1H\").sum().resample(\"1D\").mean()\n",
    "migros_FRAME_daily.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAndPlotFourier(time_series,WePlot=True):\n",
    "    '''\n",
    "    Take a  time series, and performs Fourier analysis: \n",
    "    - Computes the amplitudes of N/2 frequencies (N the number of original samples) using FFT, \n",
    "    - extract the corresponding phases. \n",
    "    It can also plot amplitudes, phases, and the resulting approx from the third main harmonics along with the \n",
    "    original signal by setting WePlot to True. The default is False. Also, the list of years is an argument of the function.\n",
    "    '''\n",
    "\n",
    "    if WePlot == True:\n",
    "        fig = plt.figure()\n",
    "\n",
    "    # current time series    \n",
    "    #time_series = Full_matrix_2017_2018[i, :]\n",
    "\n",
    "    #### ------------ TIME DOMAIN ------------------------------------------\n",
    "    \n",
    "    if WePlot==True:\n",
    "        #nameSelected = names[StationIndex]\n",
    "        #yearSelected = years[yearIndex]\n",
    "        ax1 = fig.add_subplot(2,2,1)  \n",
    "        plt.plot(range(len(time_series)), time_series, color=\"orange\" )\n",
    "        ax1.set_title(\"Time domain (Daily mean consumption)\")\n",
    "        ax1.set_xlabel(\"days\")\n",
    "        ax1.set_ylabel(\"Consumption\")\n",
    "        #ax1.set_xlabel(r\"$(a)$\")\n",
    "        #ax1.legend(frameon=True,loc='upper left')\n",
    "        #ax1.set_xlim(-10, 370)\n",
    "        \n",
    "    ### ------------ FREQUENCY DOMAIN -----------------------------------\n",
    "    # compute the fft (meaning the corresponding frequencies)\n",
    "    Y = np.fft.fft(time_series)\n",
    "    #FT[i, :] = Y\n",
    "    \n",
    "    # ---- compute AMPLITUDE with abs (computes the amplitude of the complex number as sqrt(re**2 + im**2),\n",
    "    # Multiply by 2 because we take only half of the frequencies (NYquist theorem)\n",
    "    # and NORMALIZE by sampled #data points.\n",
    "    F_amplitudes = abs(Y)*2 / len(time_series)\n",
    "    \n",
    "    # Remember that N the number of frequencies cannot be more than half of the sampling frequency!! (Nyquist theorem) Thats why there's a mirrored image\n",
    "    # when we plot the amplitudes as is. We need to plot only the first half of the freqencies, meaning N=#data/2\n",
    "    N = int(len(time_series)/2)\n",
    "    FrequenciesSelected = range(int(N))\n",
    "\n",
    "    # ---- Plot AMPLITUDE\n",
    "    if WePlot==True:\n",
    "        ax2 = fig.add_subplot(2,2,2)  \n",
    "        ax2.stem(FrequenciesSelected, F_amplitudes[:N])\n",
    "        ax2.set_title(\"Frequency domain\")\n",
    "        ax2.set_ylabel(\"Amplitude\")\n",
    "        ax2.set_xlabel(r\"$(b)$\")\n",
    "        #ax2.set_xlabel(\"frequencies\")\n",
    "        ax2.set_xlim(-1, 30)\n",
    "\n",
    "    # ---- compute PHASE of each frequency (using the angle function from numpy)\n",
    "    F_phases = np.angle(Y)\n",
    "    \n",
    "    # ---- Plot PHASE\n",
    "    if WePlot==True:\n",
    "        ax4 = fig.add_subplot(2,2,4)  \n",
    "        # plot phase \n",
    "        ax4.stem(F_phases[:30])\n",
    "        ax4.set_xlim(-1, 30)\n",
    "        ax4.set_ylim(-3.5, 3.5)\n",
    "        ax4.set_xlabel(r\"$(d)$\")\n",
    "        ax4.set_ylabel(\"Phase\")\n",
    "        #ax4.set_title(\"Phase\")\n",
    "        # y ticks\n",
    "        my_yticks = np.array([-np.pi, -0.75*np.pi, -0.5*np.pi, -0.25*np.pi,0, 0.25*np.pi, 0.5*np.pi, 0.75*np.pi, np.pi])\n",
    "        y_label = [r\"$-\\pi$\",r\"$-\\frac{3\\pi}{4}$\",r\"$-\\frac{\\pi}{2}$\", r\"$-\\frac{\\pi}{4}$\", r\"$0$\", r\"$\\frac{\\pi}{4}$\", r\"$\\frac{\\pi}{2}$\", r\"$\\frac{3\\pi}{4}$\",r\"$\\pi$\"]\n",
    "        ax4.set_yticks(my_yticks)\n",
    "        ax4.set_yticklabels(y_label, fontsize=20)\n",
    "\n",
    "    #### ------------ Fourier Estimation of the signal, back in Time domain------------------------------------------\n",
    "    \n",
    "    # harmonic index\n",
    "    #n = 1\n",
    "    # period in days\n",
    "    T = 2* 365.242189 \n",
    "    w = (2*np.pi)/(T)\n",
    "    # compute a0 (mean daily temperature through the year)\n",
    "    T0 = np.mean(time_series)\n",
    "    # NOTE that T0 = the 0th amplitude/2 !! (a0/2), I CHECKED :)\n",
    "    \n",
    "    # extract the three main harmonics, besides the 0th - the mean-, for which the amplitude (the first half of the amplitudes - Nyquist) is the maximum.\n",
    "    mainHarmIn = list(reversed( sorted(range(len(F_amplitudes[:N])), key=lambda i: F_amplitudes[:N][i])[-4:] ))[1:]\n",
    "    \n",
    "    #plot separately these four harmonics\n",
    "    if WePlot==True: \n",
    "        ax3 = fig.add_subplot(2,2,3)\n",
    "        for n in mainHarmIn:\n",
    "            ax3.plot(range(len(time_series)), T0 + np.array([ F_amplitudes[n] * np.cos(n*w*t + F_phases[n]) for t in range(len(time_series))]), linestyle='--',label=''.join([r\"$n=$\",str(n)]) )\n",
    "\n",
    "        # plot the fourier approx using these four main harmonics\n",
    "        ax3.plot(range(len(time_series)), T0 + np.array([ F_amplitudes[mainHarmIn[0]] * np.cos(mainHarmIn[0]*w*t + F_phases[mainHarmIn[0]]) for t in range(len(time_series))])\n",
    "                            + np.array([ F_amplitudes[mainHarmIn[1]] * np.cos(mainHarmIn[1]*w*t + F_phases[mainHarmIn[1]]) for t in range(len(time_series))])\n",
    "                            + np.array([ F_amplitudes[mainHarmIn[2]] * np.cos(mainHarmIn[2]*w*t + F_phases[mainHarmIn[2]]) for t in range(len(time_series))]) ,\n",
    "                            color='red', label=\"Fourier\" )\n",
    "        ax3.plot(range(len(time_series)), time_series, color=\"orange\", linestyle='-',label=\"Real\")\n",
    "        \n",
    "        lines = ax3.get_lines()\n",
    "        legend1 = plt.legend(lines[:3], [''.join([r\"$n=$\",str(n)]) for n in mainHarmIn], loc=2, frameon=True)\n",
    "        legend2 = plt.legend(lines[3:], [\"Fourier\", \"Real\"], loc=1, frameon=True)\n",
    "        ax3.add_artist(legend1)\n",
    "        ax3.add_artist(legend2)\n",
    "        \n",
    "        #ax3.legend(frameon=True,loc='upper left')\n",
    "        ax3.set_ylabel(\"Consumption\")\n",
    "        #ax3.set_xlabel(r\"$(c)$\")\n",
    "        #ax3.set_xlim(-10, 370)\n",
    "        \n",
    "        \n",
    "    # extract the main Fourier features: T0 (mean of series), main harmonics, amplitudes and phases of these harmonics\n",
    "    return([T0,mainHarmIn[0],mainHarmIn[1],mainHarmIn[2],\n",
    "                F_amplitudes[mainHarmIn[0]],F_amplitudes[mainHarmIn[1]],F_amplitudes[mainHarmIn[2]],\n",
    "              F_phases[mainHarmIn[0]],F_phases[mainHarmIn[1]],F_phases[mainHarmIn[2]]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (13, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migros_FRAME_daily.iloc[:,10].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are two annoying high peaks of electricity, which \"kill the shape\" of the loads,\n",
    "## and change dramatically the fourier features. We try to cut these peaks. \n",
    "migros_FRAME_daily_GOOD = migros_FRAME_daily[(migros_FRAME_daily.index<\"2018-09-10\") \n",
    "                                               | ( (migros_FRAME_daily.index>\"2018-09-12\") & (migros_FRAME_daily.index<\"2019-02-07\") ) \n",
    "                                               | (migros_FRAME_daily.index>\"2019-02-08\") ]\n",
    "migros_FRAME_daily_GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migros_FRAME_daily_GOOD.iloc[:,10].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migros_FRAME_daily_GOOD.iloc[:,10][\"2018-04-01\":\"2018-05-01\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=10\n",
    "ComputeAndPlotFourier(np.array(migros_FRAME_daily_GOOD.iloc[:,i]),WePlot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourier Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Station_locations_MIGROS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Fourier_features = []\n",
    "for i in range(len(np.array(migros_FRAME_daily_GOOD).T)):\n",
    "    features = ComputeAndPlotFourier(np.array(migros_FRAME_daily_GOOD).T[i, :],WePlot=False)\n",
    "    Fourier_features.append(features)\n",
    "    \n",
    "Fourier_features = np.array(Fourier_features)\n",
    "\n",
    "Fourier_features_FRAME = pd.DataFrame(Fourier_features,index = migros_FRAME_daily_GOOD.columns)\n",
    "Fourier_features_FRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Applying PCA over selected Fourier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "Fourier_features_std = scaler.fit_transform(Fourier_features)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(Fourier_features_std)\n",
    "Fourier_features_std_pca = pca.transform(Fourier_features_std)\n",
    "# check the explained variance ratio and if 3 components is enough\n",
    "print(\"Explained variance ratio for each direction:\",pca.explained_variance_ratio_)\n",
    "print(\"TOTAL explained variance:\",sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question to Dan: why not using the 5 features only? I think the students reduced it to 2 if I'm not mistaken.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Apply a clustering algorithm (Nearest neighbors, DBSCAN and OPTICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN,OPTICS\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "sns.set_style(\"white\", {'axes.grid' : True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(Fourier_features_std)\n",
    "distances, indices = nbrs.kneighbors(Fourier_features_std)\n",
    "\n",
    "plt.figure()\n",
    "ax=sns.distplot(distances[:,1],kde=False,bins=[i*1 for i in range(10)])\n",
    "plt.xlim(0,50)\n",
    "plt.xlabel(\"Distance to NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(Fourier_features)\n",
    "distances, indices = nbrs.kneighbors(Fourier_features)\n",
    "\n",
    "plt.figure()\n",
    "ax=sns.distplot(distances[:,1],kde=False,bins=[i*180 for i in range(40)])\n",
    "plt.xlim(0,2000)\n",
    "plt.xlabel(\"Distance to NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(Fourier_features_std)\n",
    "distances, indices = nbrs.kneighbors(Fourier_features_std)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.figure()\n",
    "plt.plot(distances)\n",
    "plt.ylim(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(Fourier_features)\n",
    "distances, indices = nbrs.kneighbors(Fourier_features)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.figure()\n",
    "plt.plot(distances)\n",
    "plt.ylim(0,800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "* MinPts: As a rule of thumb, a minimum minPts can be derived from the number of dimensions D in the data set, as minPts ≥ D + 1. The low value of minPts = 1 does not make sense, as then every point on its own will already be a cluster.[dubious – discuss] With minPts ≤ 2, the result will be the same as of hierarchical clustering with the single link metric, with the dendrogram cut at height ε. Therefore, minPts must be chosen at least 3. However, larger values are usually better for data sets with noise and will yield more significant clusters. As a rule of thumb, minPts = 2·dim can be used,[6] but it may be necessary to choose larger values for very large data, for noisy data or for data that contains many duplicates.[5]\n",
    "\n",
    "* ε: The value for ε can then be chosen by using a k-distance graph, plotting the distance to the k = minPts-1 nearest neighbor ordered from the largest to the smallest value.[5] Good values of ε are where this plot shows an \"elbow\":[1][6][5] if ε is chosen much too small, a large part of the data will not be clustered; whereas for a too high value of ε, clusters will merge and the majority of objects will be in the same cluster. In general, small values of ε are preferable,[5] and as a rule of thumb only a small fraction of points should be within this distance of each other. Alternatively, an OPTICS plot can be used to choose ε,[5] but then the OPTICS algorithm itself can be used to cluster the data.\n",
    "\n",
    "============================\n",
    "\n",
    "eps(float, default=0.5)\n",
    "\n",
    "    The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.\n",
    "\n",
    "min_samples(int, default=5)\n",
    "\n",
    "    The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question to Dan: dataset dimension D = 10, 5 is PCA is applied, what not choosing then minPts >= 2*D ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scanning the epsilon parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for el in [0.2 + i*3 for i in range(60)]:\n",
    "# we also played a little with eps before reaching a \"reasonable\" plot\n",
    "    clustering = DBSCAN(eps=el, min_samples=3).fit(np.array(Fourier_features))\n",
    "\n",
    "    # labels\n",
    "    DBSCAN_labels = clustering.labels_\n",
    "    #DBSCAN_labels.shape = (614,1)\n",
    "\n",
    "    # all unique classes and number of points per cluster\n",
    "    points_per_class = [ \"\".join([ str(el),\":\",str(np.sum(1*(DBSCAN_labels==el))) ]) for el in np.unique(DBSCAN_labels)]\n",
    "    print(\"eps =\",round(el,1))\n",
    "    print(points_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scanning the epsilon parameter (with Fourier_features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in [0.2 + i*0.1 for i in range(30)]:\n",
    "# we also played a little with eps before reaching a \":reasonable\" plot\n",
    "    clustering = DBSCAN(eps=el, min_samples=10,metric=\"euclidean\").fit(np.array(Fourier_features_std))\n",
    "\n",
    "    # labels\n",
    "    DBSCAN_labels = clustering.labels_\n",
    "    #DBSCAN_labels.shape = (614,1)\n",
    "\n",
    "    # all unique classes and number of points per cluster\n",
    "    points_per_class = [ \"\".join([ str(el),\":\",str(np.sum(1*(DBSCAN_labels==el))) ]) for el in np.unique(DBSCAN_labels)]\n",
    "    print(\"eps =\",round(el,1))\n",
    "    print(points_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST 1: eps=180,min_samples=2, metric=default (euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_DB = DBSCAN(eps=180,min_samples=2).fit(Fourier_features)\n",
    "# labels\n",
    "DBSCAN_labels = clust_DB.labels_\n",
    "# all unique classes and number of points per cluster\n",
    "points_per_class_DB = [ \"\".join([ str(el),\":\",str(np.sum(1*(DBSCAN_labels==el))) ]) for el in np.unique(DBSCAN_labels)]\n",
    "print(points_per_class_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST 2: eps=180,min_samples=2, metric=l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_DB = DBSCAN(eps=180,min_samples=2,metric=\"l1\").fit(Fourier_features)\n",
    "# labels\n",
    "DBSCAN_labels = clust_DB.labels_\n",
    "# all unique classes and number of points per cluster\n",
    "points_per_class_DB = [ \"\".join([ str(el),\":\",str(np.sum(1*(DBSCAN_labels==el))) ]) for el in np.unique(DBSCAN_labels)]\n",
    "print(points_per_class_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST 3: eps=180,min_samples=3, metric=l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_DB = DBSCAN(eps=180,min_samples=3,metric=\"l1\").fit(Fourier_features)\n",
    "# labels\n",
    "DBSCAN_labels = clust_DB.labels_\n",
    "# all unique classes and number of points per cluster\n",
    "points_per_class_DB = [ \"\".join([ str(el),\":\",str(np.sum(1*(DBSCAN_labels==el))) ]) for el in np.unique(DBSCAN_labels)]\n",
    "print(points_per_class_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST 4: eps=1.5,min_samples=5, metric=euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_DB = DBSCAN(eps=1.5,min_samples=5,metric=\"euclidean\").fit(Fourier_features_std)\n",
    "# labels\n",
    "DBSCAN_labels = clust_DB.labels_\n",
    "# all unique classes and number of points per cluster\n",
    "points_per_class_DB = [ \"\".join([ str(el),\":\",str(np.sum(1*(DBSCAN_labels==el))) ]) for el in np.unique(DBSCAN_labels)]\n",
    "print(points_per_class_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST 5: eps=1.7,min_samples=10, metric=euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_DB = DBSCAN(eps=1.7,min_samples=10,metric=\"euclidean\").fit(Fourier_features_std)\n",
    "# labels\n",
    "DBSCAN_labels = clust_DB.labels_\n",
    "# all unique classes and number of points per cluster\n",
    "points_per_class_DB = [ \"\".join([ str(el),\":\",str(np.sum(1*(DBSCAN_labels==el))) ]) for el in np.unique(DBSCAN_labels)]\n",
    "print(points_per_class_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migros_FRAME_daily_GOOD_2 = migros_FRAME_daily_GOOD.T\n",
    "migros_FRAME_daily_GOOD_2[\"ClusterDBSCAN\"] = np.array(DBSCAN_labels)\n",
    "\n",
    "# put back location names as index\n",
    "migros_FRAME_daily_GOOD_2.index = Station_locations_MIGROS2\n",
    "migros_FRAME_daily_GOOD_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClusteredTimeSeries(cluster_labels,ClusterName,Save=False):\n",
    "    #plt.rcParams['figure.figsize'] = (20,16)\n",
    "    fig=plt.figure()\n",
    "    for i in np.unique(cluster_labels):\n",
    "        plt.subplot(np.ceil(len(np.unique(cluster_labels))/3),3,i+2)\n",
    "        \n",
    "        # time series in the current cluster\n",
    "        cluster_frame = migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2[ClusterName]==i]\n",
    "        for el in np.array(cluster_frame):\n",
    "            plt.plot(range(len(el)),el,linewidth=2)\n",
    "        # PLOT THE MEAN series of the current cluster\n",
    "        plt.plot(range(len(np.array(cluster_frame.mean()))),np.array(cluster_frame.mean()),color=\"yellow\")\n",
    "\n",
    "        plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    \n",
    "        plt.title( \"\".join([str(len(np.array(cluster_frame))),\" series\"]) )\n",
    "    plt.show()\n",
    "    if Save==True:\n",
    "        fig.savefig(\"\".join([\"SwisscomTimeSeries_\",ClusterName,\".pdf\"]),dpi=300,bbox_inches=\"tight\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClusteredTimeSeries(DBSCAN_labels,\"ClusterDBSCAN\",Save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the locations are somehow different in clusters... and if the cluster make sense (Need MIGROS experts' input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eps=180, minSamples=2, metric=l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outliers:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==-1].index))\n",
    "\n",
    "\n",
    "print(\"cluster0:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==0].index))\n",
    "print(\"cluster1:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==1].index))\n",
    "print(\"cluster2:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==2].index))\n",
    "print(\"cluster3:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==3].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comment: We have fitness locations in the last cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eps=180, minSamples=3, metric=l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outliers:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==-1].index))\n",
    "\n",
    "\n",
    "print(\"cluster0:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==0].index))\n",
    "print(\"cluster1:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==1].index))\n",
    "print(\"cluster2:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==2].index))\n",
    "print(\"cluster3:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==3].index))\n",
    "print(\"cluster4:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==4].index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eps=180, minSamples=2, metric=euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outliers:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==-1].index))\n",
    "\n",
    "\n",
    "print(\"cluster0:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==0].index))\n",
    "print(\"cluster1:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==1].index))\n",
    "print(\"cluster2:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==2].index))\n",
    "print(\"cluster3:\")\n",
    "print(list(migros_FRAME_daily_GOOD_2[migros_FRAME_daily_GOOD_2.ClusterDBSCAN==3].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mSamples in range(1,20):\n",
    "    clust = OPTICS(min_samples=mSamples,cluster_method=\"xi\",min_cluster_size=.1).fit(Fourier_features)\n",
    "    # labels\n",
    "    OPTICS_labels = clust.labels_\n",
    "    # all unique classes and number of points per cluster\n",
    "    points_per_class = [ \"\".join([ str(el),\":\",str(np.sum(1*(OPTICS_labels==el))) ]) for el in np.unique(OPTICS_labels)]\n",
    "    print(\"min_samples = \",mSamples)\n",
    "    print(points_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_OP = OPTICS(min_samples=3,cluster_method=\"xi\",metric=\"euclidean\").fit(Fourier_features)\n",
    "# labels\n",
    "OPTICS_labels = clust_OP.labels_\n",
    "# all unique classes and number of points per cluster\n",
    "points_per_class_OP = [ \"\".join([ str(el),\":\",str(np.sum(1*(OPTICS_labels==el))) ]) for el in np.unique(OPTICS_labels)]\n",
    "print(points_per_class_OP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_OP = OPTICS(min_samples=5,cluster_method=\"xi\",metric=\"l1\").fit(Fourier_features)\n",
    "# labels\n",
    "OPTICS_labels = clust_OP.labels_\n",
    "# all unique classes and number of points per cluster\n",
    "points_per_class_OP = [ \"\".join([ str(el),\":\",str(np.sum(1*(OPTICS_labels==el))) ]) for el in np.unique(OPTICS_labels)]\n",
    "print(points_per_class_OP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
