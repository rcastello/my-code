{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Python generator for data-loading with Keras\n",
    "\n",
    "Using the `model.fit(...)` method to run the training of your Keras model is not feasible for data, which does not fit into memory. Therefore, Keras offers the `model.fit_generator(...)` method, which takes a Python generator function as dataloader. This enables you to load data \"on-the-fly\" from datasets, which do not fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/py2_virtualenv/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "This simple model serves as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=(2,)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python generator as dataloader\n",
    "\n",
    "The following generator implements a dataloader, which generates toy examples on the fly. However, you are free to implement loading batches of examples from disk in this function. Note that the generator can loop infinitely and just have to return with each `yield` statement a batch of inputs and labels, which will be used for a single gradient step during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size):\n",
    "    signal_mean = [1.0, 1.0]\n",
    "    signal_cov = [[1.0, 0.0],\n",
    "                  [0.0, 1.0]]\n",
    "    background_mean = [-1.0, -1.0]\n",
    "    background_cov = [[1.0, 0.0],\n",
    "                      [0.0, 1.0]]\n",
    "    \n",
    "    while True:\n",
    "        signal = np.random.multivariate_normal(signal_mean, signal_cov, batch_size/2)\n",
    "        background = np.random.multivariate_normal(background_mean, background_cov, batch_size/2)\n",
    "        \n",
    "        inputs = np.vstack([signal, background])\n",
    "        labels = np.vstack([np.ones((batch_size/2, 1)), np.zeros((batch_size/2, 1))])\n",
    "        \n",
    "        yield inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training\n",
    "\n",
    "As you can see below, the training is similar to the `model.fit(...)` method. As additional feature, you can load data with multiple workers in the background to a buffer in memory similar to TensorFlow's queue system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.6541 - acc: 0.5330\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5529 - acc: 0.9070\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4761 - acc: 0.9160\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4151 - acc: 0.9240\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3765 - acc: 0.9050\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3407 - acc: 0.9070\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2993 - acc: 0.9160\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2725 - acc: 0.9280\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2545 - acc: 0.9240\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2397 - acc: 0.9210\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    data_generator(batch_size=100),\n",
    "    steps_per_epoch=10,\n",
    "    epochs=10,\n",
    "    max_queue_size=10,\n",
    "    workers=1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
