{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the results of the paper *Searching for Exotic Particles in High-Energy Physics with Deep Learning*\n",
    "\n",
    "The paper *Searching for Exotic Particles in High-Energy Physics with Deep Learning* by Baldi et al. is one of the most popular papers presenting the successful usage of deep neural networks in high-energy particle physics applications.\n",
    "\n",
    "This example reproduces this important result with only about 100 lines of code using Keras.\n",
    "\n",
    "**Abstract:**\n",
    "> Collisions at high-energy particle colliders are a traditionally fruitful source of exotic particle discoveries. Finding these rare particles requires solving difficult signal-versus-background classification problems, hence machine learning approaches are often used. Standard approaches have relied on `shallow' machine learning models that have a limited capacity to learn complex non-linear functions of the inputs, and rely on a pain-staking search through manually constructed non-linear features. Progress on this problem has slowed, as a variety of techniques have shown equivalent performance. Recent advances in the field of deep learning make it possible to learn more complex functions and better discriminate between signal and background classes. Using benchmark datasets, we show that deep learning methods need no manually constructed inputs and yet improve the classification metric by as much as 8\\% over the best current approaches. This demonstrates that deep learning approaches can improve the power of collider searches for exotic particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/py2_virtualenv/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import subprocess\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "This can take a while! The final dataset has a size of about 1.2 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"HIGGS.h5\"):\n",
    "    subprocess.call(\"wget http://mlphysics.ics.uci.edu/data/higgs/HIGGS.h5\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-out the inputs and targets\n",
    "\n",
    "The inputs consist of 21 low-level and and 7 high-level variables. We want to reproduce the result of the paper with all features as inputs called `lo+hi-level` in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = h5py.File(\"HIGGS.h5\")\n",
    "inputs = np.array(file_[\"features\"])\n",
    "targets = np.array(file_[\"targets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the models\n",
    "\n",
    "The model defined below do not match exactly the setup in the paper. However, we define a shallow neural network with a single hidden layer and a deep neural network with 5 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              29000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 30,001\n",
      "Trainable params: 30,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_shallow = Sequential()\n",
    "model_shallow.add(Dense(1000, kernel_initializer=\"glorot_normal\", activation=\"tanh\",\n",
    "    input_dim=inputs.shape[1]))\n",
    "model_shallow.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "\n",
    "model_shallow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 370,201\n",
      "Trainable params: 370,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_deep = Sequential()\n",
    "model_deep.add(Dense(300, kernel_initializer=\"glorot_normal\", activation=\"relu\",\n",
    "    input_dim=inputs.shape[1]))\n",
    "model_deep.add(Dense(300, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(Dense(300, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(Dense(300, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(Dense(300, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "\n",
    "model_deep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_shallow, model_deep]:\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"nadam\",\n",
    "        metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and test data\n",
    "\n",
    "To speed up the training, we use only 10% of the full dataset for training. Feel free to enlarge this fraction to a more reasonable 50/50 or 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(\n",
    "        inputs, targets, test_size=0.90, random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare pre-processing\n",
    "\n",
    "As preprocessing, we use a standard scaler provided by the `sklearn` package. This preprocessing method takes each input and subtracts the mean and then divides by the standard-deviation so that the final distribution is centered around 0 with a width of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_input = StandardScaler()\n",
    "preprocessing_input.fit(inputs_train)\n",
    "pickle.dump(preprocessing_input, open(\"HIGGS_preprocessing.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models\n",
    "\n",
    "The following code trains the models. Here, you can experience quickly why deep-learning is heavily dependent on GPUs to speed up the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 825000 samples, validate on 275000 samples\n",
      "Epoch 1/10\n",
      "825000/825000 [==============================] - 21s 26us/step - loss: 0.5831 - acc: 0.6890 - val_loss: 0.5461 - val_acc: 0.7183\n",
      "Epoch 2/10\n",
      "825000/825000 [==============================] - 19s 23us/step - loss: 0.5398 - acc: 0.7238 - val_loss: 0.5315 - val_acc: 0.7299\n",
      "Epoch 3/10\n",
      "825000/825000 [==============================] - 20s 25us/step - loss: 0.5311 - acc: 0.7295 - val_loss: 0.5287 - val_acc: 0.7308\n",
      "Epoch 4/10\n",
      "825000/825000 [==============================] - 20s 24us/step - loss: 0.5261 - acc: 0.7328 - val_loss: 0.5255 - val_acc: 0.7335\n",
      "Epoch 5/10\n",
      "825000/825000 [==============================] - 19s 24us/step - loss: 0.5230 - acc: 0.7349 - val_loss: 0.5232 - val_acc: 0.7349\n",
      "Epoch 6/10\n",
      "825000/825000 [==============================] - 19s 23us/step - loss: 0.5215 - acc: 0.7355 - val_loss: 0.5195 - val_acc: 0.7374\n",
      "Epoch 7/10\n",
      "825000/825000 [==============================] - 20s 25us/step - loss: 0.5204 - acc: 0.7366 - val_loss: 0.5169 - val_acc: 0.7393\n",
      "Epoch 8/10\n",
      "825000/825000 [==============================] - 18s 22us/step - loss: 0.5196 - acc: 0.7366 - val_loss: 0.5191 - val_acc: 0.7376\n",
      "Epoch 9/10\n",
      "825000/825000 [==============================] - 18s 21us/step - loss: 0.5190 - acc: 0.7372 - val_loss: 0.5156 - val_acc: 0.7396\n",
      "Epoch 10/10\n",
      "825000/825000 [==============================] - 22s 27us/step - loss: 0.5185 - acc: 0.7377 - val_loss: 0.5181 - val_acc: 0.7379\n",
      "Train on 825000 samples, validate on 275000 samples\n",
      "Epoch 1/10\n",
      "825000/825000 [==============================] - 101s 123us/step - loss: 0.5400 - acc: 0.7230 - val_loss: 0.5174 - val_acc: 0.7389\n",
      "Epoch 2/10\n",
      "825000/825000 [==============================] - 93s 112us/step - loss: 0.5111 - acc: 0.7431 - val_loss: 0.5059 - val_acc: 0.7458\n",
      "Epoch 3/10\n",
      "825000/825000 [==============================] - 93s 113us/step - loss: 0.5012 - acc: 0.7494 - val_loss: 0.4972 - val_acc: 0.7528\n",
      "Epoch 4/10\n",
      "825000/825000 [==============================] - 99s 120us/step - loss: 0.4945 - acc: 0.7537 - val_loss: 0.4946 - val_acc: 0.7547\n",
      "Epoch 5/10\n",
      "825000/825000 [==============================] - 98s 118us/step - loss: 0.4897 - acc: 0.7573 - val_loss: 0.4930 - val_acc: 0.7561\n",
      "Epoch 6/10\n",
      "825000/825000 [==============================] - 95s 115us/step - loss: 0.4857 - acc: 0.7597 - val_loss: 0.4883 - val_acc: 0.7581\n",
      "Epoch 7/10\n",
      "825000/825000 [==============================] - 90s 109us/step - loss: 0.4822 - acc: 0.7616 - val_loss: 0.4876 - val_acc: 0.7578\n",
      "Epoch 8/10\n",
      "825000/825000 [==============================] - 93s 112us/step - loss: 0.4795 - acc: 0.7637 - val_loss: 0.4871 - val_acc: 0.7586\n",
      "Epoch 9/10\n",
      "825000/825000 [==============================] - 106s 128us/step - loss: 0.4764 - acc: 0.7652 - val_loss: 0.4870 - val_acc: 0.7586\n",
      "Epoch 10/10\n",
      "825000/825000 [==============================] - 100s 122us/step - loss: 0.4747 - acc: 0.7669 - val_loss: 0.4857 - val_acc: 0.7599\n"
     ]
    }
   ],
   "source": [
    "for model, name in zip([model_shallow, model_deep], [\"HIGGS_shallow.h5\", \"HIGGS_deep.h5\"]):\n",
    "    model.fit(\n",
    "            preprocessing_input.transform(inputs_train),\n",
    "            targets_train,\n",
    "            batch_size=100,\n",
    "            epochs=10,\n",
    "            validation_split=0.25)\n",
    "    model.save(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
