{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution for the XOR problem using TensorFlow\n",
    "\n",
    "The XOR problem consists of the non-linear classification problem of the crosses and circles shown in the following figure. This problem is famous because it proves in a very simple manner that a neural network with a non-linear activation function can perform a non-linear classification problem.\n",
    "\n",
    "![Problem](xor_problem.svg)\n",
    "\n",
    "In this example, we want to solve this problem with the most simple neural network using TensorFlow for the implementation. The following architecture shall serve our needs.\n",
    "\n",
    "![Net](xor_net.svg)\n",
    "\n",
    "To implement this neural network with TensorFlow, the architecture has to be translated in a computational graph. The following flowgraph displays the graph we want to implement. To do so, TensorFlow has three basic blocks, which can be connected: Placeholders, Variables and Operations.\n",
    "\n",
    "![Graph](xor_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/py2_virtualenv/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Variables are the free parameters of the graph. These can be altered during training to find the best-fitting set of parameters solving your posed problem.\n",
    "Here, we assume that we already know the best values and skip the training for now. The following code declares the variables we want to use in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for layer 1\n",
    "w1 = tf.get_variable(\"W1\", initializer=np.array([[1.0, 1.0],\n",
    "                                                 [1.0, 1.0]]))\n",
    "\n",
    "# Bias for layer 1\n",
    "b1 = tf.get_variable(\"b1\", initializer=np.array([0.0, -1.0]))\n",
    "\n",
    "# Weights for layer 2\n",
    "w2 = tf.get_variable(\"W2\", initializer=np.array([[1.0], [-2.0]]))\n",
    "\n",
    "# Bias for layer 2\n",
    "b2 = tf.get_variable(\"b2\", initializer=np.array([0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "\n",
    "Placeholders are the entry-points for your graph. Placeholders can be set when running the graph using the `feed_dict` option to feed in values from outside. Typical use-cases are feeding inputs and labels to a graph to perform the training or only inputs for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for input values\n",
    "x = tf.placeholder(tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "Operations are functions, which creates new nodes in the graph from placeholders, variables or other operations. The following cell shows the setup of the neural network model shown above by stacking the mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the hidden layer\n",
    "hidden_layer = tf.nn.relu(b1 + tf.matmul(x, w1))\n",
    "\n",
    "# Definition of the output layer\n",
    "y = b2 + tf.matmul(hidden_layer, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The session\n",
    "\n",
    "The session is the environment for your graph to be executed. It manages the devices you want to use and runs the previously defined graph or only parts of it.\n",
    "\n",
    "Note that the first operation, which (almost) always has to be executed before extracting results from the graph is the initialization of your variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run network on XOR inputs\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run inference\n",
    "    x_in = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y_out = sess.run(y, feed_dict={x:x_in})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Let us inspect the response of the neural network and whether it solved the XOR problem successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : Output\n",
      "[0 0] : 0\n",
      "[0 1] : 1\n",
      "[1 0] : 1\n",
      "[1 1] : 0\n"
     ]
    }
   ],
   "source": [
    "# Print network response\n",
    "print(\"{} : {}\".format(\"Input\", \"Output\"))\n",
    "for x_, y_ in zip(x_in, y_out):\n",
    "    print(\"{} : {}\".format(x_, int(np.squeeze(y_))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
